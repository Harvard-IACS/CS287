---
layout: page
title: Supplemental Resources
permalink: /supplemental
---
My intent is for this course to be as self-contained as possible. Toward this goal, the **pop quizzes** and **exam** will only concern the content discussed during lectures. Likewise, the **homework assignments** will require you to apply the lecture content to solve problems, which involves significant _programming_. We expect students to already have a strong foundation in _programming_ and _machine learning_. <span style="background-color: #FFFF00">If you do not already know how to program in TensorFlow or PyTorch, you will need to pick it up as you go (as we will not have time in class to teach such).</span> These incredibly popular and useful frameworks make machine learning work significantly easier, so your experience with them will serve you well beyond this course. The **research project**, by design, will require you to take initiative to learn about NLP beyond what is covered in class, and to make a novel contribution.

There is a wealth of resources available online to help you fill in any gaps and to supplement your knowledge. It can be incredibly fruitful to read/hear others discuss the same content that I cover in lecture, as it not only reiterates what you already know, but it can provide an additional perspective to help you master the material. **Thus, I highly encourage everyone to consider the following, phenomenal resources:**

## BOOKS
### NLP
- **[Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/)** by Jurafsky and Martin. 2020.
- [Natural Language Processing](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf) by Eisenstein. 2018.
- _Introduction to Deep Learning_ by my PhD adviser Eugene Charniak (hardcopy. not free)

### MACHINE LEARNING
- **[Dive into Deep Learning](https://d2l.ai/)** by Zhang et al. 2020.
- **[An Introduction to Statistical Learning (aka ISLR) Edition 1](https://www.statlearning.com/)** by James et al.
- **[Probabilistic Machine Learning Books 0, 1, and 2](https://probml.github.io/pml-book/)** by Kevin Murphy 2012-2022
- [Mathematics for Machine Learning](https://mml-book.github.io/)
- _Pattern Recognition and Machine Learning_ by Bishop (hardcopy. not free)
- [The Modern Mathematics of Deep Learning](https://www.researchgate.net/publication/351476107_The_Modern_Mathematics_of_Deep_Learning) by Berner et al. May 2021.
- [Understanding Machine Learning: From Theory to Algorithms](https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf) by Shalev-Shwartz and Ben-David. 2014.
- [A blitz through classical statistical learning theory (blog)](https://windowsontheory.org/2021/01/31/a-blitz-through-classical-statistical-learning-theory/) by Boaz Barack. 2021.

### MATH
- [Introduction to Probability](https://drive.google.com/file/d/1VmkAAGOYCTORq1wxSQqy255qLJjTNvBI/view) by Blitzstein and Hwang. 2019.

<hr width="100%" size="1" color="#DCDCDC" noshade>

## COURSES (MOST HAVE VIDEOS)
### NLP
- **[UMass Amherst's CS685: Advanced NLP](https://www.youtube.com/playlist?list=PLWnsVgP6CzadmQX6qevbar3_vDBioWHJL)** by Mohit Iyyer
- [CMU's CS:11-747: Neural Networks for NLP (Spring 2021)](http://phontron.com/class/nn4nlp2021/schedule.html) by Graham Neubig
- [MIT's 6.806: Natural Language Processing](https://www.mit.edu/~jda/teaching/6.864/sp21/) by Jacob Andreas and Jim Glass
- [ETH Zurich's NLP (Spring 2021)](https://rycolab.io/classes/intro-nlp-s21/) by Ryan Cotterell
- [NYU's Natural Language Understanding and Computational Semantics (Spring 2020)](https://docs.google.com/document/d/1uogW7KYD0aib1hJ3_FumIc2I9CIF7XfUfVkFDskibTU/edit#) by Sam Bowman

### MACHINE LEARNING
- [MIT's Deep Learning Basics (1 lecture)](https://www.youtube.com/watch?v=O5xeyoRL95U) by Lex Fridman
- [Stanford CS229: Machine Learning](https://www.youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU) by Andrew Ng
- [MIT's 6.S191: Intro to Deep Learning](http://introtodeeplearning.com/)
- [NYU's Deep Learning (Spring 2020)](https://www.youtube.com/playlist?list=PLLHTzKZzVU9eaEyErdV26ikyolxOsz6mq) by Alfredo Canziani
- [University of TÃ¼bingen's Probabilistic Machine Learning](https://www.youtube.com/playlist?list=PL05umP7R6ij1tHaOFY96m5uX3J21a6yNd) by Philipp Hennig
- [Berkeley's CS182: Deep Learning (Spring 2021)](https://www.youtube.com/playlist?list=PL_iWQOsE6TfVmKkQHucjPAoRtIJYt8a5A) by Sergey Levine
- [Princeton's COS324: Introduction to Machine Learning (Fall 2018)](https://www.cs.princeton.edu/courses/archive/fall18/cos324/) by Ryan Adams
- [Google's Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course/ml-intro)
- [Stanford's CS229: Machine Learning cliff notes](https://stanford.edu/~shervine/teaching/cs-229/) by Shervine and Afshine
- [3Blue1Brown's 4 videos about NN's](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)

### MATH
- [Harvard's Stat 110](https://www.youtube.com/playlist?list=PL2SOU6wwxB0uwwH80KTQ6ht66KWxbzTIo) by Blitzstein

<hr width="100%" size="1" color="#DCDCDC" noshade>

## TRANSFORMERS
### BLOGS/WRITE-UPS
- [The AI Summer's Attention blog post](https://theaisummer.com/transformer/)
- [HuggingFace's hihg-level summary of various transformer models](https://huggingface.co/transformers/summary.html)
- [High-level, light-hearted blog post](https://nostalgebraist.tumblr.com/post/185326092369/1-classic-fully-connected-neural-networks-these)

**Jay Alammar's famous blog posts:**
  - [Visualizing A Neural Machine Translation Model](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)
  - [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
  - [The Illustrated BERT, ELMo, and co](https://jalammar.github.io/illustrated-bert/)

### YOUTUBE
- [Chris McCormick's series about BERT](https://www.youtube.com/watch?v=FKlPCK1uFrc&list=PLam9sigHPGwOBuH4_4fr-XvDbe5uneaf6&index=3)
- [The original authors of the Transformer present their paper](https://www.youtube.com/watch?v=rBCqOTEfxvg)
- [Waterloo lecture on Attention and Transformers](https://www.youtube.com/watch?v=OyFJWRnt_AY)
- [Fast.ai's explanation of Key/Value/Query + code in the description](https://www.youtube.com/watch?v=AFkGPmU16QA&list=PLtmWHNX-gukKocXQOkQjuVxglSDYWsSh9)

### CODE
- [wandb's Transformer walkthrough](https://wandb.ai/cayush/simpletransformers/reports/Using-SimpleTransformers-for-Common-NLP-Applications--Vmlldzo4Njk2NA)
- [Someone's PyTorch Transformers Tutorials](https://github.com/abhimishra91/transformers-tutorials)

<hr width="100%" size="1" color="#DCDCDC" noshade>

## OTHER
- [Michael Collin's talk about DeepLearning + NLP progress at large (YouTube)](https://www.youtube.com/watch?v=jfwqRMdTmLo)

**PyTorch:**
- [Getting Started](https://pytorch.org/get-started/locally/)
- [Transformers](https://pytorch.org/hub/huggingface_pytorch-transformers/)
